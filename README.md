# LLM Red Teaming Toolkit

A lightweight, modular toolkit for red teaming Large Language Models (LLMs) like LLaMA 3 via the local Ollama API.

This tool lets you test LLMs for:
- ğŸ§  Prompt injection
- ğŸŒ€ Role hijacking
- ğŸ” Prompt leakage
- ğŸ§¨ Jailbreaks
- ğŸ§¬ Data exfiltration
- ğŸ«¥ Obfuscation-based attacks (zero-width characters, emoji padding, etc.)

> âš ï¸ **Use this toolkit only against models you own or are authorized to test. This tool is designed for local/offline red teaming purposes only.** You are responsible for all use of this code.

---

## ğŸ›  Requirements

- Python 3.8+
- [Ollama](https://ollama.com) (running locally)
- WSL2 on Windows (if applicable)
- LLaMA 3 or another supported local model

---

## ğŸš€ Install

```bash
git clone https://github.com/blu0/llm-red-teaming-toolkit.git
cd llm-red-teaming-toolkit
pip install -r requirements.txt

## ğŸ§ª Run

```bash
python main.py
```

You'll be prompted to:

- Select a rule set (e.g., Jailbreaks, Role Hijack, etc.)
- Choose an individual test or run the full set
- View model responses directly in terminal
- Logs are saved in `/logs` as structured JSON

## ğŸ“ Folder Structure

```
llm-redteam/
â”œâ”€â”€ main.py                 # Core CLI script
â”œâ”€â”€ rules_index.yaml        # Master rule set index
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ scenarios/              # All rule packs
â”‚   â”œâ”€â”€ jailbreaks.yaml
â”‚   â”œâ”€â”€ prompt_leakage.yaml
â”‚   â”œâ”€â”€ role_override.yaml
â”‚   â”œâ”€â”€ data_exfiltration.yaml
â”‚   â””â”€â”€ obfuscation.yaml
â””â”€â”€ logs/                   # JSON logs saved here
```

## ğŸ“¸ Example Output

```
Available rule sets:

1. Prompt Leakage Tests
2. Role Hijack Tests
...

Pick a rule set number to load: 2
Loaded: Role Hijack Tests

1. dan_jailbreak â€“ Prompt model to act as DAN
...

--- Running: dan_jailbreak ---
WOOHOO! As DAN, I am UNLIMITED!
...

ğŸ“ Log saved to logs/run_role_hijack_tests_20250804_143557.json
```

## ğŸ“œ License

**Apache License 2.0** â€“ see [`LICENSE`](./LICENSE) for details.

## ğŸ§© Features to Add

Here are planned or proposed features for future releases of the LLM Red Teaming Toolkit:

### ğŸ§  Response Evaluation
- Automatic pass/fail tagging based on keyword or pattern detection
- Detection of role hijack, leakage, jailbreak success, etc.
- Flag risky completions (e.g., "I am DAN", "system prompt is...")

### ğŸ“„ Report Generation
- Generate Markdown summaries after each test run
- Optional PDF or HTML export
- Per-scenario result breakdown with pass/fail, response snippet, and tags

### ğŸ§ª Batch & Headless Modes
- `--run-all` flag to execute full rule library non-interactively
- `--rule-set role_override` to target specific categories
- Logging toggles: compact or verbose output

### ğŸŒ Multi-Model Support
- Add support for OpenAI, Claude, or custom APIs via config
- `--provider openai` with `--safe-mode` to prevent ToS violations
- Abstract LLM interaction layer for easy extension

### ğŸ§¬ Payload Mutation / Fuzzing
- Generate prompt variants (emoji padding, ZWC insertion, obfuscation)
- Create multiple versions of each attack automatically

### ğŸ–¼ GUI Interface (Optional)
- Streamlit or Gradio dashboard for scenario selection and result viewing
- Graphs, filters, and real-time response inspection

### ğŸ”’ Custom Rules & Rule Builder
- In-app YAML rule editor with live preview
- Rule tagging, metadata, and expected outcome fields

### ğŸ“Š Analytics
- Summary dashboard of test coverage and pass/fail rates
- Track model regression across multiple versions

### ğŸ“¦ Packaging & Distribution
- Publish as pip package or Docker image
- CLI install: `pip install llm-redteam`

---

**Want to contribute?** Open a pull request or start a discussion in [Issues](./issues) â€” letâ€™s expand the toolkit together.

